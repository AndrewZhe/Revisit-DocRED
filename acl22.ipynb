{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the kappa between two annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/docred/dev.json', 'r') as fh:\n",
    "    dataset = json.load(fh)\n",
    "with open('./data/docred/rel2id.json', 'r') as fh:\n",
    "    rel2id = json.load(fh)\n",
    "id2rel = {}\n",
    "for rel, idx in rel2id.items():\n",
    "    id2rel[idx] = rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc Num: 96 Inter: 37242 Union: 38991 Annotator1 Label: 2888 Annotator2 Label: 2621\n",
      "Jaccard: 0.9551434946526122 Kappa: 0.6703530015065771\n"
     ]
    }
   ],
   "source": [
    "file_list = [249, 866, 434, 672, 184, 938, 212, 140, 546, 347, 633, 556, 205, 340, 533, 513, 130, 5, 777, 650, 20, 414, 543, 419, 813, 561, 52, 310, 698, 314, 61, 961, 198, 714, 974, 746, 753, 881, 377, 564, 270, 835, 371, 471, 692, 560, 905, 411, 38, 721, 297, 559, 803, 649, 955, 409, 380, 976, 815, 500, 480, 660, 438, 454, 718, 306, 996, 154, 620, 772, 498, 155, 787, 332, 770, 767, 196, 981, 115, 722, 609, 289, 231, 959, 124, 17, 222, 653, 214, 181, 940, 426, 685, 244, 807, 496]\n",
    "\n",
    "name1 = 'annotator1'\n",
    "name2 = 'annotator2'\n",
    "\n",
    "same, tot, hqz, yey = 0, 0, 0, 0\n",
    "pred1, pred2 = [], []\n",
    "\n",
    "for file in file_list:\n",
    "    data = dataset[file-1]\n",
    "    \n",
    "    infile1 = './data/annotation/{}/{}.pkl'.format(name1, file - 1)\n",
    "    infile2 = './data/annotation/{}/{}.pkl'.format(name2, file - 1)\n",
    "    \n",
    "    data1 =  pk.load( open( infile1, \"rb\" ) )\n",
    "    data2 = pk.load(open(infile2, 'rb'))\n",
    "    \n",
    "    data['pred_labels'] = []\n",
    "\n",
    "    pred_labels1 = dict()\n",
    "    pred_labels2 = dict()\n",
    "\n",
    "    for key in data1:\n",
    "        val1, val2 = data1[key], data2[key]\n",
    "        for r_idx in range(97):\n",
    "            rel = id2rel[r_idx]\n",
    "            r1, r2 = val1[r_idx], val2[r_idx]\n",
    "            if r1 in [2,3]:\n",
    "                pred_labels1[(key[0], key[1], rel)] = \"\"\n",
    "            if r2 in [2,3]:\n",
    "                pred_labels2[(key[0], key[1], rel)] = \"\"\n",
    "    union = set(pred_labels1.keys()) | set(pred_labels2.keys())\n",
    "    non_nan = set()\n",
    "    for key in union:\n",
    "        non_nan.add((key[0], key[1]))\n",
    "    nan_num = len(set(data1.keys()) - non_nan)\n",
    "    for key in union:\n",
    "\n",
    "        rel = key[2]\n",
    "        if key in pred_labels1 and key in pred_labels2:\n",
    "            data['pred_labels'].append({'h': key[0], 't': key[1], 'r': rel, 'evidence': [],})\n",
    "            pred1.append(rel2id[rel])\n",
    "            pred2.append(rel2id[rel])\n",
    "        elif key in pred_labels1 and key not in pred_labels2:\n",
    "            data['pred_labels'].append({'h': key[0], 't': key[1], 'r': rel, 'evidence': [],})\n",
    "            pred1.append(rel2id[rel])\n",
    "            pred2.append(0)\n",
    "        else:\n",
    "            data['pred_labels'].append({'h': key[0], 't': key[1], 'r': rel, 'evidence': [],})\n",
    "            pred1.append(0)\n",
    "            pred2.append(rel2id[rel])\n",
    "    pred1 += [0] * nan_num\n",
    "    pred2 += [0] * nan_num\n",
    "\n",
    "same = np.sum(np.equal(pred1, pred2))\n",
    "tot = len(pred1)\n",
    "pred_num1 = len(np.nonzero(pred1)[0])\n",
    "pred_num2 = len(np.nonzero(pred2)[0])\n",
    "kappa = cohen_kappa_score(pred1, pred2)\n",
    "\n",
    "print (\"Doc Num:\", len(file_list), \"Inter:\", same, \"Union:\", tot, \"Annotator1 Label:\", pred_num1, \"Annotator2 Label:\", pred_num2)\n",
    "print (\"Jaccard:\", same / tot, \"Kappa:\", kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sub dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Scratch、revise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/annotation/reverse_rules.json', 'r') as fh:\n",
    "    reverse_rules = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('./data/annotation/discussed'):\n",
    "    try:\n",
    "        no = int(file.split('.')[0])\n",
    "        if no + 1 not in file_list:\n",
    "            os.remove('./data/annotation/discussed/{}'.format(file))\n",
    "    except:\n",
    "        os.remove('./data/annotation/discussed/{}'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc Num: 96 Inter: 867 Union: 3308 Scratch: 2961 DocRED: 1214\n",
      "Jaccard: 0.2620918984280532 Kappa: 0.19712609763651767\n"
     ]
    }
   ],
   "source": [
    "file_list = [249, 866, 434, 672, 184, 938, 212, 140, 546, 347, 633, 556, 205, 340, 533, 513, 130, \n",
    "             5, 777, 650, 20, 414, 543, 419, 813, 561, 52, 310, 698, 314, 61, 961, 198, 714, 974, \n",
    "             746, 753, 881, 377, 564, 270, 835, 371, 471, 692, 560, 905, 411, 38, 721, 297, 559, \n",
    "             803, 649, 955, 409, 380, 976, 815, 500, 480, 660, 438, 454, 718, 306, 996, 154, 620, \n",
    "             772, 498, 155, 787, 332, 770, 767, 196, 981, 115, 722, 609, 289, 231, 959, 124, 17, \n",
    "             222, 653, 214, 181, 940, 426, 685, 244, 807, 496,]\n",
    "\n",
    "name1 = 'discussed'\n",
    "name2 = 'docred'\n",
    "\n",
    "same, tot, hqz, yey = 0, 0, 0, 0\n",
    "pred1, pred2 = [], []\n",
    "new_dataset = []\n",
    "for file in file_list:\n",
    "    data = dataset[file-1]\n",
    "    infile1 = './data/annotation/{}/{}.pkl'.format(name1, file - 1)\n",
    "    \n",
    "    data1 =  pk.load( open( infile1, \"rb\" ) )\n",
    "    \n",
    "    data['pred_labels'] = []\n",
    "\n",
    "    pred_labels1 = dict()\n",
    "    pred_labels2 = dict()\n",
    "\n",
    "    for key in data1:\n",
    "        val1 = data1[key]\n",
    "        for r_idx in range(97):\n",
    "            rel = id2rel[r_idx]\n",
    "            r1 = val1[r_idx]\n",
    "            if r1 in [2,3]:\n",
    "                pred_labels1[(key[0], key[1], rel)] = \"\"\n",
    "                if rel in reverse_rules and len(reverse_rules[rel]) == 1:\n",
    "                    reverse_rel = reverse_rules[rel][0]\n",
    "                    if (key[1], key[0], reverse_rel) not in pred_labels1:\n",
    "                        pred_labels1[(key[1], key[0], reverse_rel)] = \"PostReverse\"  \n",
    "\n",
    "    for label in data['labels']:\n",
    "        pred_labels2[(label['h'], label['t'], label['r'])] = \"\"\n",
    "                                \n",
    "    union = set(pred_labels1.keys()) | set(pred_labels2.keys())\n",
    "    non_nan = set()\n",
    "    for key in union:\n",
    "        non_nan.add((key[0], key[1]))\n",
    "    nan_num = len(set(data1.keys()) - non_nan)\n",
    "    for key in union:\n",
    "\n",
    "        rel = key[2]\n",
    "        if key in pred_labels1 and key in pred_labels2:\n",
    "            data['pred_labels'].append({'h': key[0], 't': key[1], 'r': rel, 'evidence': [], 'pred_label': 'TP', 'notes': \"\"})\n",
    "            pred1.append(rel2id[rel])\n",
    "            pred2.append(rel2id[rel])\n",
    "        elif key in pred_labels1 and key not in pred_labels2:\n",
    "            data['pred_labels'].append({'h': key[0], 't': key[1], 'r': rel, 'evidence': [], 'pred_label': 'FP', 'notes': \"\"})\n",
    "            pred1.append(rel2id[rel])\n",
    "            pred2.append(0)\n",
    "        else:\n",
    "            data['pred_labels'].append({'h': key[0], 't': key[1], 'r': rel, 'evidence': [], 'pred_label': 'FN', 'notes': \"\"})\n",
    "            pred1.append(0)\n",
    "            pred2.append(rel2id[rel])\n",
    "\n",
    "    new_dataset.append(data)\n",
    "\n",
    "with open('./data/annotation/Diff_Scratch_Revise.json'.format(name1, name2), 'w') as fh:\n",
    "    json.dump(new_dataset, fh, indent=2)\n",
    "same = np.sum(np.equal(pred1, pred2))\n",
    "tot = len(pred1)\n",
    "pred_num1 = len(np.nonzero(pred1)[0])\n",
    "pred_num2 = len(np.nonzero(pred2)[0])\n",
    "kappa = cohen_kappa_score(pred1, pred2)\n",
    "\n",
    "print (\"Doc Num:\", len(file_list), \"Inter:\", same, \"Union:\", tot, \"Scratch:\", pred_num1, \"DocRED:\", pred_num2)\n",
    "print (\"Jaccard:\", same / tot, \"Kappa:\", kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dataset = []\n",
    "revise_dataset = []\n",
    "for data in new_dataset:\n",
    "    new_data = copy.deepcopy(data)\n",
    "    data['labels'] = [label for label in data['pred_labels'] if label['pred_label'] in ['TP', 'FP']]\n",
    "    del data['pred_labels']\n",
    "    scratch_dataset.append(data)\n",
    "    new_data['labels'] = [label for label in new_data['pred_labels'] if label['pred_label'] in ['TP', 'FN']]\n",
    "    del new_data['pred_labels']\n",
    "    revise_dataset.append(new_data)\n",
    "with open('./data/docred/valid_scratch.json', 'w') as fh:\n",
    "    json.dump(scratch_dataset, fh, indent=2)\n",
    "with open('./data/docred/valid_revise.json', 'w') as fh:\n",
    "    json.dump(revise_dataset, fh, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设docred中标注没有错误，将revise中存在的rel全部放入scratch中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/docred/valid_scratch.json', 'r') as fh:\n",
    "    scratch_dataset = json.load(fh)\n",
    "with open('./data/docred/valid_revise.json', 'r') as fh:\n",
    "    revise_dataset = json.load(fh)\n",
    "for data1, data2 in zip(scratch_dataset, revise_dataset):\n",
    "    label1 = [(label['h'], label['t'], label['r']) for label in data1['labels']]\n",
    "    label2 = [(label['h'], label['t'], label['r']) for label in data2['labels']]\n",
    "\n",
    "    for label in set(label2) - set(label1):\n",
    "        data1['labels'].append({'h': label[0], 't': label[1], 'r': label[2]})\n",
    "\n",
    "with open('./data/docred/valid_scratch.json', 'w') as fh:\n",
    "    json.dump(scratch_dataset, fh, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Recommend Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25170\n"
     ]
    }
   ],
   "source": [
    "tot_labels = 0\n",
    "\n",
    "with open('./data/docred/init.json', 'r') as fh:\n",
    "    init_dataset = json.load(fh)\n",
    "with open('./data/docred/dev.json', 'r') as fh:\n",
    "    dev_dataset = json.load(fh)\n",
    "with open('./data/docred/dev.json', 'r') as fh:\n",
    "    valid_recommend = json.load(fh)\n",
    "for idx, data in enumerate(dev_dataset):\n",
    "    for init_data in init_dataset:\n",
    "        if init_data['title'] == data['title']:\n",
    "            valid_recommend[idx] = init_data\n",
    "            break\n",
    "# for i in [0] + list(range(2, 1000)):\n",
    "#     vertex_old2new = {}\n",
    "#     for old_idx, old_nodes in enumerate(valid_recommend[i]['vertexSet']):\n",
    "#         for new_idx, new_nodes in enumerate(dev_dataset[i]['vertexSet']):\n",
    "#             if len(set([(t['sent_id'], t['pos'][0], t['pos'][1]) for t in old_nodes]) & set([(t['sent_id'], t['pos'][0], t['pos'][1]) for t in new_nodes])) > 0:\n",
    "#                 vertex_old2new[old_idx] = new_idx\n",
    "#                 break\n",
    "#     labels = []\n",
    "#     label_set = set()\n",
    "#     for label in valid_recommend[i]['init']:\n",
    "#         if label['h'] in vertex_old2new and label['t'] in vertex_old2new and vertex_old2new[label['h']] != vertex_old2new[label['t']]:\n",
    "#             if (vertex_old2new[label['h']], vertex_old2new[label['t']], label['r']) not in label_set:\n",
    "#                 labels.append({'h': vertex_old2new[label['h']], 't': vertex_old2new[label['t']], 'r': label['r']})\n",
    "#                 label_set.add((vertex_old2new[label['h']], vertex_old2new[label['t']], label['r']))   \n",
    "# \n",
    "for i in [0] + list(range(2, 1000)):\n",
    "    vertex_old2new = defaultdict(list)\n",
    "    for old_idx, old_nodes in enumerate(valid_recommend[i]['vertexSet']):\n",
    "        for new_idx, new_nodes in enumerate(dev_dataset[i]['vertexSet']):\n",
    "            if len(set([(t['sent_id'], t['pos'][0], t['pos'][1]) for t in old_nodes]) & set([(t['sent_id'], t['pos'][0], t['pos'][1]) for t in new_nodes])) > 0:\n",
    "                # vertex_old2new[old_idx].append(new_idx)\n",
    "                vertex_old2new[old_idx] = [new_idx]\n",
    "                break\n",
    "                \n",
    "    labels = []\n",
    "    label_set = set()\n",
    "    for label in valid_recommend[i]['init']:\n",
    "        if label['h'] in vertex_old2new and label['t'] in vertex_old2new:\n",
    "            for h in vertex_old2new[label['h']]:\n",
    "                for t in vertex_old2new[label['t']]:\n",
    "                    if h == t:\n",
    "                        continue\n",
    "                    if (h, t, label['r']) not in label_set:\n",
    "                        labels.append({'h': h, 't': t, 'r': label['r']})\n",
    "                        label_set.add((h, t, label['r'])) \n",
    "                \n",
    "    tot_labels += len(set([(label['h'], label['t'], label['r']) for label in labels]))\n",
    "    assert len(set([(label['h'], label['t'], label['r']) for label in labels])) == len(label_set)\n",
    "\n",
    "    dev_dataset[i]['labels'] = labels\n",
    "\n",
    "print (tot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/docred/valid_revise.json', 'r') as fh:\n",
    "    revise_dataset = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [249, 866, 434, 672, 184, 938, 212, 140, 546, 347, 633, 556, 205, 340, 533, 513, 130, \n",
    "             5, 777, 650, 20, 414, 543, 419, 813, 561, 52, 310, 698, 314, 61, 961, 198, 714, 974, \n",
    "             746, 753, 881, 377, 564, 270, 835, 371, 471, 692, 560, 905, 411, 38, 721, 297, 559, \n",
    "             803, 649, 955, 409, 380, 976, 815, 500, 480, 660, 438, 454, 718, 306, 996, 154, 620, \n",
    "             772, 498, 155, 787, 332, 770, 767, 196, 981, 115, 722, 609, 289, 231, 959, 124, 17, \n",
    "             222, 653, 214, 181, 940, 426, 685, 244, 807, 496,]\n",
    "valid_recommend = [dev_dataset[idx-1] for idx in file_list]\n",
    "for revise_data, recommend_data in zip(revise_dataset, valid_recommend):\n",
    "    revise_labels = [(label['h'], label['t'], label['r']) for label in revise_data['labels']]\n",
    "    recommend_labels = [(label['h'], label['t'], label['r']) for label in recommend_data['labels']]\n",
    "    recommend_labels = set(recommend_labels) & set(revise_labels) # 只考虑在revise阶段保留的triple\n",
    "    recommend_data['labels'] = [{'h': l[0], 't': l[1], 'r': l[2]} for l in recommend_labels]\n",
    "with open('./data/docred/valid_recommend.json', 'w') as fh:\n",
    "    json.dump(valid_recommend, fh, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_recommend = [len(data['labels']) for data in valid_recommend]\n",
    "l_revise = [len(data['labels']) for data in revise_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 16, 12, 3, 9, 8, 18, 8, 13]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_recommend[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6, 18, 13, 3, 9, 10, 18, 8, 13]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_revise[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = {}\n",
    "with open('./data/docred/valid_scratch.json', 'r') as fh:\n",
    "    valid_dataset['scratch'] = json.load(fh)\n",
    "with open('./data/docred/valid_recommend.json', 'r') as fh:\n",
    "    valid_dataset['recommend'] = json.load(fh)\n",
    "with open('./data/docred/valid_revise.json', 'r') as fh:\n",
    "    valid_dataset['revise'] = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset['scratch_recommend'] = []\n",
    "valid_dataset['revise_recommend'] = []\n",
    "valid_dataset['scratch_revise'] = []\n",
    "\n",
    "for scratch_data, revise_data, recommend_data in zip(valid_dataset['scratch'], valid_dataset['revise'], valid_dataset['recommend']):\n",
    "    scratch_labels = set([(l['h'], l['t'], l['r']) for l in scratch_data['labels']])\n",
    "    revise_labels = set([(l['h'], l['t'], l['r']) for l in revise_data['labels']])\n",
    "    recommend_labels = set([(l['h'], l['t'], l['r']) for l in recommend_data['labels']])\n",
    "\n",
    "    data = copy.deepcopy(revise_data)\n",
    "    data['labels'] = [{'h': l[0], 't': l[1], 'r': l[2]} for l in scratch_labels - recommend_labels]\n",
    "    valid_dataset['scratch_recommend'].append(data)\n",
    "\n",
    "    data = copy.deepcopy(revise_data)\n",
    "    data['labels'] = [{'h': l[0], 't': l[1], 'r': l[2]} for l in scratch_labels - revise_labels]\n",
    "    valid_dataset['scratch_revise'].append(data)\n",
    "\n",
    "    data = copy.deepcopy(revise_data)\n",
    "    data['labels'] = [{'h': l[0], 't': l[1], 'r': l[2]} for l in revise_labels - recommend_labels]\n",
    "    valid_dataset['revise_recommend'].append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular vs Unpopular relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/docred/rel2id.json', 'r') as fh:\n",
    "    rel2id = json.load(fh)\n",
    "with open('./data/wikidata/Popular Relations.txt' ,'r') as fh:\n",
    "    lines = fh.readlines()\n",
    "popular_relations = [line.strip().split('\\t')[0].split(' ')[-1][1:-1] for line in lines]\n",
    "popular_relations = set(popular_relations) & set(list(rel2id))\n",
    "unpopular_relations = [rel for rel in rel2id if rel not in popular_relations and rel != 'Na']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {\n",
    "    '# Instance': [],\n",
    "    '# Pop Rel': [],\n",
    "    '# Unpop Rel': [],\n",
    "    '$popularity_{max}$': [],\n",
    "    '$popularity_{min}$': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: recommend, #Instances: 1167, #Popular Rel: 659(56.5%), #Unpopular Rel: 508(43.5%)\n",
      "Data: revise, #Instances: 1214, #Popular Rel: 676(55.7%), #Unpopular Rel: 538(44.3%)\n",
      "Data: scratch, #Instances: 3308, #Popular Rel: 1615(48.8%), #Unpopular Rel: 1693(51.2%)\n",
      "Data: revise_recommend, #Instances: 47, #Popular Rel: 17(36.2%), #Unpopular Rel: 30(63.8%)\n",
      "Data: scratch_recommend, #Instances: 2141, #Popular Rel: 956(44.7%), #Unpopular Rel: 1185(55.3%)\n",
      "Data: scratch_revise, #Instances: 2094, #Popular Rel: 939(44.8%), #Unpopular Rel: 1155(55.2%)\n"
     ]
    }
   ],
   "source": [
    "for data_name in ['recommend', 'revise', 'scratch', 'revise_recommend', 'scratch_recommend', 'scratch_revise']:\n",
    "    dataset = valid_dataset[data_name]\n",
    "    ins_with_rel = defaultdict(list)\n",
    "    for data in dataset:\n",
    "        for label in data['labels']:\n",
    "            ins_with_rel[label['r']].append((data['title'], label['h'], label['t'], rel2id[label['r']]))\n",
    "    pop_rel_instances = []\n",
    "    unpop_rel_instances = []\n",
    "    for rel in popular_relations:\n",
    "        pop_rel_instances += ins_with_rel[rel]\n",
    "    for rel in unpopular_relations:\n",
    "        unpop_rel_instances += ins_with_rel[rel]\n",
    "\n",
    "    pop_rel_num = len(set(pop_rel_instances))\n",
    "    unpop_rel_num = len(set(unpop_rel_instances))\n",
    "\n",
    "    print (\"Data: {}, #Instances: {}, #Popular Rel: {}({:.1f}%), #Unpopular Rel: {}({:.1f}%)\".format(\n",
    "        data_name,\n",
    "        pop_rel_num + unpop_rel_num,\n",
    "        pop_rel_num, pop_rel_num / (pop_rel_num + unpop_rel_num) * 100,\n",
    "        unpop_rel_num, unpop_rel_num / (pop_rel_num + unpop_rel_num) * 100,\n",
    "    ))\n",
    "    dt['# Instance'].append(pop_rel_num + unpop_rel_num)\n",
    "    dt['# Pop Rel'].append(\"{} ({}%)\".format(pop_rel_num, format(100 * pop_rel_num / (pop_rel_num + unpop_rel_num), \".1f\") ))\n",
    "    dt['# Unpop Rel'].append(\"{} ({}%)\".format(unpop_rel_num, format(100 * unpop_rel_num / (pop_rel_num + unpop_rel_num), \".1f\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/wikidata/final_id2popular.json', 'r') as fh:\n",
    "    id2popular = json.load(fh)\n",
    "with open('./data/wikidata/final_wiki_ids.json', 'r') as fh:\n",
    "    wiki_ids = json.load(fh)\n",
    "\n",
    "with open('./data/docred/dev.json', 'r') as fh:\n",
    "    dev_dataset = json.load(fh)\n",
    "doc_titles = [data['title'] for data in dev_dataset]\n",
    "title2id = {}\n",
    "for fid, title in enumerate(doc_titles):\n",
    "    title2id[title] = fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/huangqz/repos/binary_document_re/GAIN-dsn/code/wiki/ent2feature.json', 'r') as fh:\n",
    "#     ent2feature = json.load(fh)\n",
    "\n",
    "# with open('/home/huangqz/repos/binary_document_re/GAIN-dsn/code/wiki/real_ent2feature.json', 'r') as fh:\n",
    "#     real_ent2feature = json.load(fh)\n",
    "\n",
    "\n",
    "# id2popular = {}\n",
    "# for qid in ent2feature:\n",
    "#     id2popular[qid] = sum(len(rel) for rel in ent2feature[qid]['rel'])\n",
    "# for qid in real_ent2feature:\n",
    "#     id2popular[qid] = sum(len(rel) for rel in real_ent2feature[qid]['rel'])\n",
    "\n",
    "# with open('./data/wikidata/final_id2popular.json', 'w') as fh:\n",
    "#     json.dump(id2popular, fh, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/huangqz/repos/binary_document_re/GAIN-dsn/code/wiki/real_wiki_ids.json', 'r') as fh:\n",
    "#     wiki_ids = json.load(fh)\n",
    "\n",
    "# with open('/home/huangqz/repos/binary_document_re/GAIN-dsn/code/wiki/wiki_ids.json', 'r') as fh:\n",
    "#     wiki_ids_backup = json.load(fh)\n",
    "\n",
    "# for doc_wiki_ids, doc_wiki_ids_backup in zip(wiki_ids, wiki_ids_backup):\n",
    "#     for i, ent_wiki_ids in enumerate(doc_wiki_ids):\n",
    "#         if len(ent_wiki_ids) == 0:\n",
    "#             doc_wiki_ids[i] = doc_wiki_ids_backup[i]\n",
    "# wiki_ids[1] = wiki_ids_backup[1]\n",
    "\n",
    "# with open('./data/wikidata/final_wiki_ids.json', 'w') as fh:\n",
    "#     json.dump(wiki_ids, fh, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_popular_with_title_hts(title, h, t, merge_method='max'):\n",
    "    doc_id = title2id[title]\n",
    "    h_wiki_ids = wiki_ids[doc_id][int(h)]\n",
    "    t_wiki_ids = wiki_ids[doc_id][int(t)]\n",
    "\n",
    "    h_popularity = [id2popular[qid] for qid in h_wiki_ids if qid in id2popular]\n",
    "    t_popularity = [id2popular[qid] for qid in t_wiki_ids if qid in id2popular]\n",
    "\n",
    "    h_popularity = -1 if len(h_popularity) == 0 else max(h_popularity)\n",
    "    t_popularity = -1 if len(t_popularity) == 0 else max(t_popularity)\n",
    "\n",
    "    if merge_method == 'max':\n",
    "        return max(h_popularity, t_popularity)\n",
    "    elif merge_method == 'min':\n",
    "        return min(h_popularity, t_popularity)\n",
    "    elif merge_method == 'ave':\n",
    "        return (h_popularity + t_popularity) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: recommend, #Instances: 1167, #Max Popularity: 294.4, #Min Popularity: 85.2\n",
      "Data: revise, #Instances: 1214, #Max Popularity: 291.5, #Min Popularity: 84.4\n",
      "Data: scratch, #Instances: 3308, #Max Popularity: 266.3, #Min Popularity: 67.4\n",
      "Data: revise_recommend, #Instances: 47, #Max Popularity: 221.3, #Min Popularity: 66.0\n",
      "Data: scratch_recommend, #Instances: 2141, #Max Popularity: 251.0, #Min Popularity: 57.7\n",
      "Data: scratch_revise, #Instances: 2094, #Max Popularity: 251.7, #Min Popularity: 57.5\n"
     ]
    }
   ],
   "source": [
    "for data_name in ['recommend', 'revise', 'scratch', 'revise_recommend', 'scratch_recommend', 'scratch_revise']:\n",
    "    dataset = valid_dataset[data_name]\n",
    "    max_popularity = [get_popular_with_title_hts(data['title'], label['h'], label['t'], 'max') for data in dataset for label in data['labels']]\n",
    "    min_popularity = [get_popular_with_title_hts(data['title'], label['h'], label['t'], 'min') for data in dataset for label in data['labels']]\n",
    "    print (\"Data: {}, #Instances: {}, #Max Popularity: {:.1f}, #Min Popularity: {:.1f}\".format(\n",
    "        data_name, len(max_popularity), np.mean(max_popularity), np.mean(min_popularity)))\n",
    "    dt['$popularity_{max}$'].append(format(np.mean(max_popularity), \".1f\"))\n",
    "    dt['$popularity_{min}$'].append(format(np.mean(min_popularity), \".1f\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [\"$mathbf{D_{Recommend}}$\", \"$mathbf{D_{Revise}}$\", \"$mathbf{D_{Scratch}}$\", \"$mathbf{D_{Revise} - D_{Recommend}}$\", \"$mathbf{D_{Scratch} - D_{Recommend}}$\", \"$mathbf{D_{Scratch} - D_{Revise}}$\"]\n",
    "df.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrllll}\n",
      "\\toprule\n",
      "{} &  \\# Instance &     \\# Pop Rel &   \\# Unpop Rel & $popularity_{max}$ & $popularity_{min}$ \\\\\n",
      "\\midrule\n",
      "$\\mathbf{D_{Recommend}}$               &        1167 &   659 (56.5\\%) &   508 (43.5\\%) &              294.4 &               85.2 \\\\\n",
      "$\\mathbf{D_{Revise}}$                  &        1214 &   676 (55.7\\%) &   538 (44.3\\%) &              291.5 &               84.4 \\\\\n",
      "$\\mathbf{D_{Scratch}}$                 &        3308 &  1615 (48.8\\%) &  1693 (51.2\\%) &              266.3 &               67.4 \\\\\n",
      "$\\mathbf{D_{Revise} - D_{Recommend}}$  &          47 &    17 (36.2\\%) &    30 (63.8\\%) &              221.3 &               66.0 \\\\\n",
      "$\\mathbf{D_{Scratch} - D_{Recommend}}$ &        2141 &   956 (44.7\\%) &  1185 (55.3\\%) &              251.0 &               57.7 \\\\\n",
      "$\\mathbf{D_{Scratch} - D_{Revise}}$    &        2094 &   939 (44.8\\%) &  1155 (55.2\\%) &              251.7 &               57.5 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex().replace(\"\\\\$\", \"$\").replace(\"\\\\_\", \"_\").replace(\"\\\\{\", \"{\").replace(\"\\\\}\", \"}\").replace(\"mathbf\", \"\\mathbf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b067e2bae65440c58a63d0e5883b00886d5bd6abd73a93319623dcccd768640"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
